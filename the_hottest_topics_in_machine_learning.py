# -*- coding: utf-8 -*-
"""The Hottest Topics in Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xiqXiSpkr_PsbsXSdDa_D3QGpFV-FM_t
"""

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.pyplot
import re
import wordcloud
from sklearn.decomposition import LatentDirichletAllocation as LDA
import warnings

paper = pd.read_csv("/content/drive/MyDrive/Kodlar/Veri setleri/papers.csv")

print(paper.head())

paper.info()

paper.drop(["id","event_type","pdf_name"],axis= 1,inplace= True)

paper.head()

# Commented out IPython magic to ensure Python compatibility.
groups = paper.groupby("year")

# Determine the size of each group
count = groups.size()

# Visualise the counts as a bar plot

# %matplotlib inline
count.plot(kind='bar');

paper['title_processed'] = paper['title'].map(lambda x: re.sub('[,\.!?]', '', x))

paper['title_processed'] = paper['title_processed'].str.lower()

print(paper.head())

"""To verify whether the preprocessing has been done correctly, we can create a word cloud from the titles of research articles. This will give us a visual representation of the most common words. Visualization is key to knowing if we are still on the right track! In addition, it allows us to verify whether we need additional pre-processing before further analyzing the text data.

Python has lots of open libraries! Instead of trying to develop a method for creating word clouds ourselves, we will use Andreas Mueller's wordcloud library.
"""

!pip install pytagcloud

!pip install simplejson

import wordcloud
# Join the different processed titles together.
long_string = ' '.join(paper['title_processed'])

# Create a WordCloud object
wordcloud = wordcloud.WordCloud()

# Generate a word cloud
wordcloud.generate(long_string)

# Visualize the word cloud
wordcloud.to_image()

# Helper function
def plot_10_most_common_words(count_data, count_vectorizer):

    words = count_vectorizer.get_feature_names_out()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]

    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words))

    plt.bar(x_pos, counts,align='center')
    plt.xticks(x_pos, words, rotation=90)
    plt.xlabel('words')
    plt.ylabel('counts')
    plt.title('10 most common words')
    plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the processed titles
count_data = tfidf_vectorizer.fit_transform(paper['title_processed'])

# Visualise the 10 most common words
plot_10_most_common_words(count_data, tfidf_vectorizer)

warnings.simplefilter("ignore", DeprecationWarning)

#Helper Function
def print_topic(model,count_vectorizer,n_top_words):
  words = count_vectorizer.get_feature_names_out()
  for topic_idx,topic in enumerate(model.components_):
    print("\nTopic #%d:" % topic_idx)
    print(" ".join([words[i]
                    for i in topic.argsort()[:-n_top_words - 1:-1]]))

number_topics = 10
number_words = 10
# Tweak the two parameters below (use int values below 15)
lda = LDA(n_components=number_topics)

# Create and fit the LDA model
lda.fit(count_data)

# Print the topics found by the LDA model
print("Topics found via LDA:")
print_topic(lda,count_vectorizer,number_words)